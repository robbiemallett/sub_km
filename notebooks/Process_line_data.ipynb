{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from collections import namedtuple\n",
    "import os\n",
    "\n",
    "s_line_dir = '../NP_transects/'\n",
    "\n",
    "month_nums = {'jan':1,\n",
    "              'feb':2,\n",
    "              'fab':2,\n",
    "              'mch':3,\n",
    "              'mar':3,\n",
    "              'apr':4,\n",
    "              'may':5,\n",
    "              'jun':6,\n",
    "              'jul':7,\n",
    "              'aug':8,\n",
    "              'spt':9,\n",
    "              'sep':9,\n",
    "              'oct':10,\n",
    "              'nov':11,\n",
    "              'dec':12}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process depths from 'raw data'\n",
    "\n",
    "### This goes through all the original text files and parses them straight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_d = collections.defaultdict(dict)\n",
    "\n",
    "filenames =  os.listdir(s_line_dir)\n",
    "filenames = [item for item in filenames if 'xlsx' not in item]\n",
    "\n",
    "for filename in filenames:\n",
    "    \n",
    "#     print(filename)\n",
    "    \n",
    "    # Get year\n",
    "    \n",
    "    partitions = filename.partition('.')\n",
    "    \n",
    "    year = 1900 + int(partitions[-1])\n",
    "    \n",
    "    # Get station number\n",
    "    \n",
    "    station_num = int(partitions[0].partition('_')[-1])\n",
    "    \n",
    "    snowline_data = pd.read_csv(s_line_dir+filename,\n",
    "                            delim_whitespace=True,\n",
    "                            skiprows=1,\n",
    "                            index_col=False)\n",
    "    \n",
    "#     print(snowline_data)\n",
    "    \n",
    "    first_row = list(snowline_data.iloc[0])\n",
    "    \n",
    "    first_row.insert(0, first_row.pop())\n",
    "    \n",
    "    snowline_data.loc[0,:] = first_row\n",
    "\n",
    "    snowline_data.drop(columns='row',inplace=True)\n",
    "    \n",
    "    snowline_data.loc[0,:] = [int(re.findall('\\d+',string)[0]) for string in snowline_data.loc[0]]\n",
    "\n",
    "    \n",
    "    col_translator = {}\n",
    "    \n",
    "    for month, day in zip(list(snowline_data.columns), list(snowline_data.loc[0,:])):\n",
    "        \n",
    "        try:\n",
    "            col_translator[month] = datetime.date(month=month_nums[month[:3].lower()],\n",
    "                                            year=year,\n",
    "                                            day = day)\n",
    "        except Exception as e:\n",
    "            \n",
    "            if (day == 31) & (month[:3] == 'jun'):\n",
    "                day = 30\n",
    "                col_translator[month] = datetime.date(month=month_nums[month[:3].lower()],\n",
    "                                        year=year,\n",
    "                                        day = day)\n",
    "                \n",
    "    snowline_data.rename(columns=col_translator,inplace=True)\n",
    "    \n",
    "    snowline_data.replace([-9.9,-9,999, -99.0], np.nan, inplace=True)\n",
    "    \n",
    "#     print(snowline_data)\n",
    "\n",
    "    line_d[station_num][year] = snowline_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also made an excel file of all the transects to make things easier for people. The following cell just counts the number of transects in that file and verifies that there are 499 in there to match the result of the 'raw' processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.ExcelFile(f'{s_line_dir}/transect_depths.xlsx')\n",
    "\n",
    "x.sheet_names\n",
    "\n",
    "counter=0\n",
    "\n",
    "for sn in x.sheet_names:\n",
    "    \n",
    "    df = pd.read_excel(f'{s_line_dir}/transect_depths.xlsx',sheet_name=sn)\n",
    "    \n",
    "    counter+=(df.shape[1]-1)\n",
    "    \n",
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of transects in the dictionary that was generated from the 'raw' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "for key1 in line_d.keys():\n",
    "    \n",
    "    for key2 in line_d[key1].keys():\n",
    "    \n",
    "        counter+=len(line_d[key1][key2].keys())\n",
    "    \n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "\n",
    "# Collect data from individual years\n",
    "    \n",
    "line_depths = {}\n",
    "\n",
    "for station_num in line_d.keys():\n",
    "\n",
    "    list_of_dfs = []\n",
    "    \n",
    "    if line_d[station_num]:\n",
    "        \n",
    "        for year in line_d[station_num]:\n",
    "            \n",
    "            list_of_dfs.append(line_d[station_num][year])\n",
    "\n",
    "        result = pd.concat(list_of_dfs, axis=1, join='outer').astype(np.float32)\n",
    "\n",
    "        result = result.replace([-9.9,-9,999, -99.0], np.nan)\n",
    "\n",
    "        line_depths[station_num] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 dict_keys([1957, 1958, 1959])\n",
      "8 dict_keys([1962, 1959, 1961])\n",
      "22 dict_keys([1974, 1976, 1975, 1978, 1980, 1979, 1981, 1977, 1982])\n",
      "27 dict_keys([1986, 1987, 1984, 1985])\n",
      "18 dict_keys([1971, 1970, 1968])\n",
      "25 dict_keys([1982, 1984, 1983, 1981])\n",
      "29 dict_keys([1988])\n",
      "13 dict_keys([1965, 1964, 1966])\n",
      "15 dict_keys([1966, 1967])\n",
      "31 dict_keys([1990, 1991, 1988, 1989])\n",
      "10 dict_keys([1962, 1963])\n",
      "24 dict_keys([1980, 1979, 1978])\n",
      "23 dict_keys([1977, 1978])\n",
      "16 dict_keys([1968, 1971, 1969, 1970])\n",
      "12 dict_keys([1963, 1964, 1965])\n",
      "11 dict_keys([1962, 1963])\n",
      "30 dict_keys([1987, 1990, 1989, 1991, 1988])\n",
      "19 dict_keys([1971, 1970, 1972])\n",
      "26 dict_keys([1984, 1985, 1983, 1986])\n",
      "9 dict_keys([1969])\n",
      "14 dict_keys([1965])\n",
      "20 dict_keys([1972, 1970, 1971])\n",
      "5 dict_keys([1955])\n"
     ]
    }
   ],
   "source": [
    "for key in line_d:\n",
    "    print(key, line_d[key].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(line_depths, open('../pickles/line_depths_dict.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
